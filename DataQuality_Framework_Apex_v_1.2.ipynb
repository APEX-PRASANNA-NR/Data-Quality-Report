{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required packages and modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connector Packages:\n",
    "from snowflake.snowpark.session import Session\n",
    "from pyspark.sql import functions as F\n",
    "import openpyxl\n",
    "\n",
    "#ML Packages:\n",
    "from config import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "from datetime import datetime\n",
    "\n",
    "#Other Packages:\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = input(\"DATABASE NAME : \").upper()\n",
    "schema_name = input(\"SCHEMA NAME : \").upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {}\n",
    "excel_columns = [\"COLUMN_NAME\",]\n",
    "\n",
    "workbook = openpyxl.Workbook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snowpark_session():\n",
    "  conn_params = {\n",
    "      \"account\" : \"pr91731-production_northeurope\",\n",
    "      \"user\" : input(\"Username : \"),\n",
    "      \"password\" : input(\"Password : \"),\n",
    "      \"database\" : db_name,     \n",
    "      \"role\" : \"DATAENGINEER\",\n",
    "      \"warehouse\" : \"INGESTION_WH\"\n",
    "  }\n",
    "\n",
    "  session = Session.builder.configs(conn_params).create()\n",
    "\n",
    "  return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected Successfully\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    snowpark = snowpark_session()\n",
    "    print(\"Connected Successfully\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect with the Snowflake to read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_excel_file(worksheet,summary):\n",
    "    row = 2\n",
    "    \n",
    "    #Adding heading\n",
    "    for excel_column_name in excel_columns:\n",
    "        # print(excel_column_name,end=\" \")\n",
    "        worksheet.cell(row = 1, column = excel_columns.index(excel_column_name)+1, value = excel_column_name)\n",
    "    # print()\n",
    "    for column in summary.keys():\n",
    "        # print(column,end=\" > \")\n",
    "        worksheet.cell(\n",
    "            row = row, \n",
    "            column = 1,\n",
    "            value = column.lower()\n",
    "        )\n",
    "        \n",
    "        calculations = summary[column]\n",
    "        \n",
    "        for metric in calculations.keys():\n",
    "            column = excel_columns.index(metric) + 1\n",
    "            # print(\"\\n\\t\",column,\" - \",calculations[metric],end=\"\")\n",
    "            if(isinstance(calculations[metric],str)):\n",
    "                cal = calculations[metric].lower()\n",
    "            else:\n",
    "                cal = calculations[metric]\n",
    "            \n",
    "            worksheet.cell(\n",
    "                row = row, \n",
    "                column = column, \n",
    "                value = cal\n",
    "            )\n",
    "        row += 1\n",
    "        # print()\n",
    "    return worksheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(report,indent = 0,start_delimiter = \"[\",end_delimiter = \"]\",seperator = \" -> \"):    \n",
    "    if(isinstance(report,list)):\n",
    "        val = \"\"\n",
    "        val += start_delimiter\n",
    "        for value in report:\n",
    "            if(report.index(value) == len(report)-1):\n",
    "                val += str(value)\n",
    "            else:\n",
    "                val += str(value)+\", \"\n",
    "        val+= end_delimiter\n",
    "        print(indent+val)\n",
    "    \n",
    "    elif(isinstance(report,str)):\n",
    "        print('\"'+report+'\"')\n",
    "    \n",
    "    \n",
    "    elif(isinstance(report,dict)):\n",
    "        for key,value in report.items():\n",
    "            space = indent*'\\t'\n",
    "            print(f\"{space}{key} {seperator} \",end=\"\")\n",
    "            if(isinstance(value,dict) or isinstance(value,list) or isinstance(value,tuple) or isinstance(value,set)):\n",
    "                indent += 1\n",
    "                print(\"\")\n",
    "                show(indent = indent,report = value)\n",
    "                indent -=1\n",
    "            elif(isinstance(value,str)):\n",
    "                show(indent = indent,report = value)\n",
    "            else:\n",
    "                print(f\"{value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_validity_check_list(table_name):\n",
    "    try:\n",
    "        validity_workbook = openpyxl.load_workbook(\"DQ_Validity.xlsx\")\n",
    "        \n",
    "        if(table_name in validity_workbook.sheetnames):\n",
    "            \n",
    "            worksheet = validity_workbook[table_name]\n",
    "            \n",
    "            check_list = {}\n",
    "            for row in worksheet.iter_rows(min_row=2, values_only=True):\n",
    "                outer_key = row[0]\n",
    "                inner_dict = {}\n",
    "                for col_num, value in enumerate(row[2:], start=3):\n",
    "                    if value is not None:\n",
    "                        header_value = worksheet.cell(row=1, column=col_num).value\n",
    "                        inner_dict[header_value] = value\n",
    "                check_list[outer_key] = inner_dict\n",
    "                \n",
    "            validity_workbook.close()\n",
    "        return check_list\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_validity(data_set, validation_dict):\n",
    "    \n",
    "    results = {}\n",
    "\n",
    "    for col, check_dict in validation_dict.items():\n",
    "        result = check_column(col, data_set, check_dict)\n",
    "        results[col] = result\n",
    "\n",
    "    return results\n",
    "\n",
    "def check_column(col, data_set, check_dict):\n",
    "    \n",
    "    total_rows = data_set.shape[0]\n",
    "    failed_validations = 0\n",
    "\n",
    "    for key, check in check_dict.items():\n",
    "        if key.lower() == \"format\":\n",
    "            if check == \"alnum\":\n",
    "                min_length = check_dict.get(\"min_length\", None)\n",
    "                max_length = check_dict.get(\"max_length\", None)\n",
    "                failed_validations += count_failed_validations(col, data_set, min_length, max_length)\n",
    "\n",
    "        # Add more checks as needed\n",
    "\n",
    "    success_percentage = ((total_rows - failed_validations) / total_rows) * 100 if total_rows > 0 else 0\n",
    "    return {\"success_percentage\": success_percentage}\n",
    "\n",
    "def count_failed_validations(col, data_set, min_length, max_length):\n",
    "\n",
    "    failed_validations = 0\n",
    "\n",
    "    for value in data_set[col]:\n",
    "        if not isinstance(value, str):\n",
    "            failed_validations += 1\n",
    "        elif not value.isalnum() or (min_length is not None and len(value) < min_length) or (max_length is not None and len(value) > max_length):\n",
    "            failed_validations += 1\n",
    "\n",
    "    return failed_validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_quality_check(table_name):\n",
    "  try:\n",
    "    \n",
    "    ddl_sql = f\"DESCRIBE TABLE {db_name}.{schema_name}.{table_name}\"\n",
    "    ddl = snowpark.sql(ddl_sql).collect()\n",
    "    \n",
    "    #collecting the data structure(Datatype)/schematics of the table for identifying the \n",
    "    data_sql = f\"SELECT * FROM {db_name}.{schema_name}.{table_name}\"\n",
    "    data = snowpark.sql(data_sql)\n",
    "    \n",
    "    for row in ddl:\n",
    "      \n",
    "      col = row[\"name\"]\n",
    "      summary[col] = {}\n",
    "      \n",
    "      summary[col][\"DATA_TYPE\"] = row[\"type\"]\n",
    "      if(\"DATA_TYPE\" not in excel_columns):\n",
    "        excel_columns.append(\"DATA_TYPE\")\n",
    "      \n",
    "      #FINDING THE NULL COUNT IN EACH COLUMN\n",
    "      summary[col][\"NULL_COUNT\"] = data.filter(data['\"'+col+'\"'].isNull()).count()\n",
    "      if(\"NULL_COUNT\" not in excel_columns):\n",
    "        excel_columns.append(\"NULL_COUNT\")\n",
    "\n",
    "      #FINDING THE TOTAL COUNT IN EACH COLUMN\n",
    "      summary[col][\"TOTAL_COUNT\"] = data.count()\n",
    "      if(\"TOTAL_COUNT\" not in excel_columns):\n",
    "        excel_columns.append(\"TOTAL_COUNT\")\n",
    "      \n",
    "      #FINDING THE NOT-NULL COUNT IN EACH COLUMN\n",
    "      summary[col][\"NOT_NULL_COUNT\"] = summary[col][\"TOTAL_COUNT\"] - summary[col][\"NULL_COUNT\"]\n",
    "      if(\"NOT_NULL_COUNT\" not in excel_columns):\n",
    "        excel_columns.append(\"NOT_NULL_COUNT\")\n",
    "      \n",
    "      #FINDING THE UNIQUE(DISTINCT) VALUE COUNT IN EACH COLUMN\n",
    "      summary[col][\"UNIQUE_COUNT\"] = data.select('\"'+col+'\"').distinct().count() - 1\n",
    "      if(\"UNIQUE_COUNT\" not in excel_columns):\n",
    "        excel_columns.append(\"UNIQUE_COUNT\")\n",
    "        \n",
    "      #FINDING THE REPEARING/DUPLICATE VALUE COUNT IN EACH COLUMN EXCLUDING THE UNIQUE COUNT\n",
    "      summary[col][\"DUPLICATE\"] = summary[col][\"NOT_NULL_COUNT\"] - summary[col][\"UNIQUE_COUNT\"]\n",
    "      if(\"DUPLICATE\" not in excel_columns):\n",
    "        excel_columns.append(\"DUPLICATE\")\n",
    "      \n",
    "      #Completeness\n",
    "      summary[col][\"COMPLETENESS\"] =\"{:.2f}\".format((summary[col][\"NOT_NULL_COUNT\"] / summary[col][\"TOTAL_COUNT\"])*100)\n",
    "      if \"COMPLETENESS\" not in excel_columns:\n",
    "        excel_columns.append(\"COMPLETENESS\")\n",
    "        \n",
    "      summary[col][\"UNIQUENESS\"] =\"{:.2f}\".format((summary[col][\"NULL_COUNT\"] / summary[col][\"TOTAL_COUNT\"])*100)\n",
    "      if \"UNIQUENESS\" not in excel_columns:\n",
    "        excel_columns.append(\"UNIQUENESS\")\n",
    "      \n",
    "      \n",
    "      summary[col][\"ACCURACY\"] =\"{:.2f}\".format((summary[col][\"NULL_COUNT\"] / summary[col][\"TOTAL_COUNT\"])*100)\n",
    "      if \"ACCURACY\" not in excel_columns:\n",
    "        excel_columns.append(\"ACCURACY\")\n",
    "    \n",
    "      check_list = read_validity_check_list(table_name)\n",
    "      \n",
    "      if(check_list):\n",
    "        col_check_list = check_list[col.lower()]\n",
    "\n",
    "        print(data['\"'+col+'\"'],\" : \",col_check_list)\n",
    "        # check_validity(data['\"'+col+'\"'],col_check_list)\n",
    "        \n",
    "      else:\n",
    "        summary[col][\"Error\"] = \"Cannot perform validity as checklist not available\"\n",
    "        if(\"Error\" not in excel_columns):\n",
    "          excel_columns.append(\"Error\")\n",
    "      \n",
    "  except Exception as e:\n",
    "    print(\"Error in calculating common data types - \",e)\n",
    "    \n",
    "  return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialising the required variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table selected : ACCOUNTS_RECC_REOCC\n",
      "Table selected : COHORT_STAGING_MASTER_DATA\n",
      "Column[\"Company\"]  :  {'format': 'string'}\n",
      "Column[\"Client Account\"]  :  {'format': 'string'}\n",
      "Column[\"SF/FF Client Account ID\"]  :  {'format': 'alnum', 'min_length': 4, 'max_length': 8}\n",
      "Column[\"Entity Name\"]  :  {'format': 'string'}\n",
      "Column[\"FF Entity Code\"]  :  {'format': 'alnum', 'min_length': 4, 'max_length': 8}\n",
      "Column[\"Customer\"]  :  {'format': 'alnum', 'min_length': 4, 'max_length': 8}\n",
      "Column[\"Client Group Name\"]  :  {'format': 'string'}\n",
      "Column[\"Region\"]  :  {'format': 'string'}\n",
      "Column[\"GLA\"]  :  {'format': 'alnum'}\n",
      "Column[\"Acquired date\"]  :  {'format': 'num', 'min_length': 4, 'max_length': 4, 'max_decimals': 0}\n",
      "Column[\"DIM 1\"]  :  {'format': 'string', 'min_length': 4, 'max_length': 4}\n",
      "Column[\"Year\"]  :  {'format': 'num', 'min_length': 4, 'max_length': 4}\n",
      "Column[\"Currency\"]  :  {'format': 'string', 'min_length': 3, 'max_length': 3}\n",
      "Column[\"Jan\"]  :  {'format': 'num', 'max_decimals': 2}\n",
      "Column[\"Feb\"]  :  {'format': 'num', 'max_decimals': 2}\n",
      "Column[\"Mar\"]  :  {'format': 'num', 'max_decimals': 2}\n",
      "Column[\"Apr\"]  :  {'format': 'num', 'max_decimals': 2}\n",
      "Column[\"May\"]  :  {'format': 'num', 'max_decimals': 2}\n",
      "Column[\"Jun\"]  :  {'format': 'num', 'max_decimals': 2}\n",
      "Column[\"Jul\"]  :  {'format': 'num', 'max_decimals': 2}\n",
      "Column[\"Aug\"]  :  {'format': 'num', 'max_decimals': 2}\n",
      "Column[\"Sep\"]  :  {'format': 'num', 'max_decimals': 2}\n",
      "Column[\"Oct\"]  :  {'format': 'num', 'max_decimals': 2}\n",
      "Column[\"Nov\"]  :  {'format': 'num', 'max_decimals': 2}\n",
      "Column[\"Dec\"]  :  {'format': 'num', 'max_decimals': 2}\n",
      "Column[\"Total Revenue\"]  :  {'format': 'num', 'max_decimals': 2}\n",
      "Column[\"CompanyCode\"]  :  {'format': 'string', 'min_length': 4, 'max_length': 6, 'special characters': 'n'}\n",
      "Table selected : FACT_PROPHIX_RECON\n",
      "File created...\n"
     ]
    }
   ],
   "source": [
    "all_tables = snowpark.sql(f\"SHOW TABLES IN {db_name}.{schema_name}\").collect()\n",
    "table_names = [row[\"name\"] for row in all_tables]\n",
    "\n",
    "download_folder_path = os.path.expanduser(\"~\" + os.path.sep + \"Downloads\")\n",
    "report_path = os.path.join(download_folder_path, 'Data_Quality_Report.xlsx')\n",
    "\n",
    "for table_name in table_names:\n",
    "    print(f\"Table selected : {table_name}\")\n",
    "    \n",
    "    if(table_name == 'COHORT_STAGING_MASTER_DATA'):\n",
    "        \n",
    "        if(len(workbook.sheetnames) == 1):\n",
    "            new_worksheet = workbook.active\n",
    "            new_worksheet.title = table_name\n",
    "        else:\n",
    "            new_worksheet = workbook.create_sheet(title = table_name)\n",
    "        \n",
    "        report_summary = data_quality_check(table_name)\n",
    "        # show(report = report_summary)    \n",
    "        new_worksheet = write_to_excel_file(new_worksheet,report_summary,)\n",
    "    \n",
    "downloads_folder = os.path.expanduser(\"~\" + os.path.sep + \"Downloads\")\n",
    "file_path = os.path.join(downloads_folder,'Data_Quality_Report_'+schema_name+\"(\"+str(datetime.now().strftime(\"%d-%m-%Y\"))+').xlsx')\n",
    "workbook.save(file_path)\n",
    "print(\"File created...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "Worksheet not created for :  set()\n",
      "Unwanted worksheet : {'COHORT_STAGING_MASTER_DATA'}\n"
     ]
    }
   ],
   "source": [
    "#Validation\n",
    "\n",
    "workbook = load_workbook(file_path)\n",
    "\n",
    "# Get the number of sheets\n",
    "number_of_sheets = len(workbook.sheetnames)\n",
    "\n",
    "snowpark.sql(\"USE {db_name};\")\n",
    "snowflake_tables = list(snowpark.sql(\"SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = '{schema_name}';\").collect())\n",
    "\n",
    "excel_sheets = list(workbook.sheetnames)\n",
    "\n",
    "print(len(snowflake_tables),len(excel_sheets))\n",
    "\n",
    "print(\"Worksheet not created for : \",set(snowflake_tables)-set(excel_sheets))\n",
    "print(\"Unwanted worksheet :\", set(excel_sheets)-set(snowflake_tables))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dqenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
